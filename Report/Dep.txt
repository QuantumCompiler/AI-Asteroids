\subsection{Training Results}

For this project, there were a total of 5 different training sessions that were run. From this, five agents were created. Here is how they were created:

\begin{itemize}
	\item \textbf{Agent 10} - 10 episodes of training.
	\item \textbf{Agent 20} - 20 episodes of training.
	\item \textbf{Agent 30} - 30 episodes of training.
	\item \textbf{Agent 40} - 40 episodes of training.
	\item \textbf{Agent 50} - 50 episodes of training.
\end{itemize}

For each agent, the current episode score and the running average were updated and store in a csv file that was saved to the `\textbf{Training Results/}' directory. The running average and current episode
score for each agent were plotted with the use \texttt{matplotlib} to visualize the training process. The results for Agent 10 can be seen below.

\begin{center}
	\includegraphics[width=0.5\textwidth]{Figures/Agent 10 Training.png}
\end{center}

Agent 20 can be seen below.

\begin{center}
	\includegraphics[width=0.5\textwidth]{Figures/Agent 20 Training.png}
\end{center}

Agent 30 can be seen below.

\begin{center}
	\includegraphics[width=0.5\textwidth]{Figures/Agent 30 Training.png}
\end{center}

Agent 40 can be seen below.

\begin{center}
	\includegraphics[width=0.5\textwidth]{Figures/Agent 40 Training.png}
\end{center}

And lastly, Agent 50 can be seen below.

\begin{center}
	\includegraphics[width=0.5\textwidth]{Figures/Agent 50 Training.png}
\end{center}

Comparing all of these together, the final results of the training for each agent can be visualized in the figure below.

\begin{center}
	\includegraphics[width=0.5\textwidth]{Figures/All Agent Training.png}
\end{center}

The final averages for each agent can be seen in the table below.

\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Agent} & \textbf{Final Average} \\
		\hline
		Agent 10 & 915.0 \\ \hline
		Agent 20 & 702.0 \\ \hline
		Agent 30 & 742.33 \\ \hline
		Agent 40 & 711.0 \\ \hline
		Agent 50 & 766.4 \\ \hline
	\end{tabular}
\end{center}

From first glance, it seems like Agent 10 performed the best out of all the agents. However, when we go to test the agents in the environment, we see a slightly different story.

\section{Evaluation}

Once the agents were trained, they were then evaluated in the Asteroids-v5 environment. The evaluation process consisted of the user selecting which agent they would like to evaluate, and then having
it play through one game in the environment. The evaluation process was done by running the `\textbf{lib/main.py}' file and then selecting the agent that was to be evaluated.

\subsection{Evaluation Results}

Similar to the training of the agents, the episode number and the subsequent score were stored in a csv file that was saved to the `\textbf{Evaluation Results/}' directory. After evaluating the agents,
the following results were produced.

\begin{center}
	\includegraphics[width=0.5\textwidth]{Figures/All Agent Testing.png}
\end{center}

The above image depicts the results of an agent paired with the average score of its training. The results for each agent after evaluating and comparing them to their training averages can be seen in the table below.

\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Agent} & \textbf{Final Average} & \textbf{Evaluation Score} \\
		\hline
		Agent 10 & 915.0 & \color{red}{140.0} \\ \hline
		Agent 20 & 702.0 & \color{red}{460.0} \\ \hline
		Agent 30 & 742.33 & \color{green}{880.0} \\ \hline
		Agent 40 & 711.0 & \color{green}{830.0} \\ \hline
		Agent 50 & 766.4 & \color{red}{110.0} \\ \hline
	\end{tabular}
\end{center}

From the aforementioned table and figure, we can see that there were two agents that performed better than their training average, Agents 30 and 40. The other agents, Agents 10, 20, and 50, all performed
worse than when they were being trained.